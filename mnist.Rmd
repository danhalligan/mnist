---
output: html_document
title: MNIST using tensorflow and neural nets
---

```{r, message = FALSE}
library(tidyverse)
library(ggplot2)
library(keras)
library(tensorflow)
library(yardstick)
library(tidymodels)
library(workflows)
set.seed(123)
```

## Exploration

```{r}
train <- read_csv("data/train.csv")
```

Let's plot the first 20 hand drawn numbers.

```{r}
head(train, n = 20) %>%
  mutate(id = row_number()) %>%
  pivot_longer(cols = -c(label, id)) %>%
  mutate(
    pixel = as.numeric(str_extract(name, "\\d+")),
    x = pixel %% 28,
    y = 28 - pixel %/% 28
  ) %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile() +
  facet_wrap(~ label + id)
```

What is the distribution of numbers?

```{r}
p <- train %>% ggplot(aes(label)) + geom_barchart()
```

## Modelling

### Reshaping

Firstly, let's set aside some samples from the training dataset for testing.

```{r}
index <- sample(x = nrow(train), size = nrow(train) * 0.9)

data_train <- train[index, ]
data_test <- train[-index, ]

performance <- function(pred, act) {
  act <- as.factor(act)
  pred <- as.factor(pred)
  data.frame(
    Accuracy = accuracy_vec(act, pred),
    Recall = sens_vec(act, pred),
    Precision = precision_vec(act, pred),
    F1 = f_meas_vec(act, pred)
  ) %>%
  mutate_all(scales::percent, accuracy = 0.01)
}
```

We'll try some basic models: random forest and KNN that can do well on this
data set. However, they are computationally intensive, so we cannot really
train or even use the full data set without substantial time usage. Instead,
we'll try with a small sample of data to demonstrate that they could work in
principle.

### Random forest

```{r}
data_train$label <- factor(data_train$label)

rf_mod <- rand_forest(trees = 1000, mtry = 28, min_n = 10) %>%
  set_engine("ranger") %>%
  set_mode("classification") %>%
  fit(label ~ ., data = data_train[1:5000, ])

performance(predict(rf_mod, data_test)[[1]], data_test$label)
```

### KNN

```{r}
knn_mod <- nearest_neighbor(neighbors = 50) %>%
  set_engine("kknn") %>%
  set_mode("classification") %>%
  fit(label ~ ., data = data_train[1:5000, ])

performance(predict(knn_mod, data_test)[[1]], data_test$label)
```

Single layer perceptron

```{r}
fit <- mlp(epochs = 5, hidden_units = 100, dropout = 0.1) %>%
  set_mode("classification") %>%
  set_engine("keras") %>%
  fit(label ~ ., data = data_train)

performance(predict(fit, data_test)[[1]], data_test$label)
```


## Multilayer perceptron

```{r}
prep <- function(data) as.matrix(select(data, -label)) / 255

train_x <- prep(data_train)
test_x <- prep(data_test)

train_x_keras <- array_reshape(train_x, dim(train_x))
test_x_keras <- array_reshape(test_x, dim(test_x))

train_y <- to_categorical(data_train$label, num_classes = 10)
test_y <- data_test$label

tf$random$set_seed(123)

model <- keras_model_sequential(name = "MLP") %>%
  layer_dense(512, "sigmoid", input_shape = ncol(train_x), name = "h1") %>%
  layer_dense(128, "sigmoid", name = "h2") %>%
  layer_dense(32, "sigmoid", name = "h3") %>%
  layer_dense(16, "sigmoid", name = "h4") %>%
  layer_dense(10, "softmax", name = "Output") %>%
  compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_adam(lr = 0.001),
    metrics = "accuracy"
  )

train_history <- model %>%
  fit(
    x = train_x_keras,
    y = train_y,
    epochs = 15,
    batch_size = 32,
    validation_split = 0.1,
    verbose = 1
  )

plot(train_history)

performance(predict_classes(model, test_x_keras), data_test$label)

submit_mnist <- read.csv("data/test.csv")
test_x <- submit_mnist %>% as.matrix()/255
test_x <- array_reshape(test_x, dim(test_x))
pred_test <- predict_classes(model, test_x)

data.frame(
    ImageId = 1:nrow(submit_mnist),
    Label = pred_test
  ) %>%
  write.csv("submission.csv", row.names = FALSE)
```


## Convolution NN

```{r}
train_x_keras <- array_reshape(train_x, dim = c(nrow(train_x), 28, 28, 1))
test_x_keras <- array_reshape(test_x, dim = c(nrow(test_x), 28, 28, 1))

tensorflow::tf$random$set_seed(123)

model <- keras_model_sequential(name = "CNN") %>%
  layer_conv_2d(32, c(4, 4),
    padding = "same", activation = "relu",
    input_shape = c(28, 28, 1)
  ) %>%
  layer_batch_normalization() %>%
  layer_max_pooling_2d(pool_size = c(3, 3)) %>%
  layer_dropout(0.25) %>%
  layer_conv_2d(32, c(4, 4),
    padding = "same", activation = "relu",
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(3, 3)) %>%
  layer_conv_2d(32, c(4, 4),
    padding = "same", activation = "relu",
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(3, 3)) %>%
  layer_dropout(0.25) %>%
  layer_flatten() %>%
  layer_dense(256, "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(0.25) %>%
  layer_dense(10, "softmax", name = "Output")

model

model %>%
  compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_adam(lr = 0.0005),
    metrics = "accuracy"
  )

train_history <- model %>%
  fit(
    x = train_x_keras,
    y = train_y,
    epochs = 30,
    batch_size = 32,
    validation_split = 0.1,
    verbose = 1
  )

plot(train_history, smooth = FALSE) + geom_line()

performance(predict_classes(model, test_x_keras), data_test$label)

submit_mnist <- read.csv("data/test.csv")
test_x <- submit_mnist %>% as.matrix()/255
test_x <- array_reshape(test_x, dim = c(nrow(test_x), 28, 28, 1))
pred_test <- predict_classes(model, test_x)

data.frame(
    ImageId = 1:nrow(submit_mnist),
    Label = pred_test
  ) %>%
  write.csv("submission_cnn.csv", row.names = FALSE)

```



Data augmentation

```{r}
augment_img <- function(img, mode) {
  if (mode == 0) {
    cbind(img[, -1], rep(0, 28))
  } else if (mode == 1) {
    cbind(rep(0, 28), img[, -28])
  } else if (mode == 2) {
    rbind(img[-1, ], rep(0, 28))
  } else {
    rbind(rep(0, 28), img[-28, ])
  }
}

augment_set <- function(data, mode) {
  for (i in 1:dim(data)[1]) {
    data[i, , ,] <- augment_img(data[i, , ,], mode = 1)
  }
  data
}

library(abind)
train_x_augmented <- abind(
  train_x_keras,
  augment_set(train_x_keras, mode = 0),
  augment_set(train_x_keras, mode = 1),
  augment_set(train_x_keras, mode = 2),
  augment_set(train_x_keras, mode = 3),
  along = 1
)

train_y_augmented <- rbind(train_y, train_y, train_y, train_y, train_y)

model <- keras_model_sequential(name = "CNN") %>%
  layer_conv_2d(32, c(4, 4),
    padding = "same", activation = "relu",
    input_shape = c(28, 28, 1)
  ) %>%
  layer_batch_normalization() %>%
  layer_max_pooling_2d(pool_size = c(3, 3)) %>%
  layer_dropout(0.25) %>%
  layer_conv_2d(32, c(4, 4),
    padding = "same", activation = "relu",
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(3, 3)) %>%
  layer_conv_2d(32, c(4, 4),
    padding = "same", activation = "relu",
    input_shape = c(28, 28, 1)
  ) %>%
  layer_max_pooling_2d(pool_size = c(3, 3)) %>%
  layer_dropout(0.25) %>%
  layer_flatten() %>%
  layer_dense(256, "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(0.25) %>%
  layer_dense(10, "softmax", name = "Output")

model

model %>%
  compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_adam(lr = 0.0005),
    metrics = "accuracy"
  )

train_history <- model %>%
  fit(
    x = train_x_augmented,
    y = train_y_augmented,
    epochs = 30,
    batch_size = 32,
    validation_split = 0.1,
    verbose = 1
  )

plot(train_history, smooth = FALSE) + geom_line()

performance(predict_classes(model, test_x_keras), data_test$label)

submit_mnist <- read.csv("data/test.csv")
test_x <- submit_mnist %>% as.matrix()/255
test_x <- array_reshape(test_x, dim = c(nrow(test_x), 28, 28, 1))
pred_test <- predict_classes(model, test_x)

data.frame(
    ImageId = 1:nrow(submit_mnist),
    Label = pred_test
  ) %>%
  write.csv("submission_cnn.csv", row.names = FALSE)
